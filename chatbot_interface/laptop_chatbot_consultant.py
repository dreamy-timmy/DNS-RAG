import streamlit as st

import sys
import os

# –î–æ–±–∞–≤–∏—Ç—å –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ sys.path
sys.path.append(os.path.dirname(os.path.abspath(__file__)) + "/../")

from model.llm import LLM
llm = LLM()
llm.set_prompt()



st.title("üí¨ –ß–∞—Ç–±–æ—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –Ω–æ—É—Ç–±—É–∫–æ–≤")
st.caption("–í–∞—à –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –≤—ã–±–æ—Ä—É –Ω–æ—É—Ç–±—É–∫–∞ –Ω–∞ –≤—Å–µ —Å–ª—É—á–∞–∏ –∂–∏–∑–Ω–∏")

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "–î–æ–±—Ä–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫! –ö–∞–∫ —è –º–æ–≥—É –ø–æ–º–æ—á—å?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])
    
if question := st.chat_input('–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å:'):
    #llm_instance = LLM()
    st.session_state.messages.append({"role": "user", "content": question})
    st.chat_message("user").write(question)
    response = llm.invoke(question)['answer'] #llm_instance.prompt(prompt)  
    msg = '–û—Ç–≤–µ—Ç'#response. ?
    st.session_state.messages.append({"role": "assistant", "content": response})
    st.chat_message("assistant").markdown(response)
